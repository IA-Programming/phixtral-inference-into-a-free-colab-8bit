{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IA-Programming/phixtral-inference-into-a-free-colab-8bit/blob/main/phixtral-2x4_8bit-streamlit-interface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OA7htfttwb-"
      },
      "source": [
        "# Setting up the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwXOfKPptjDG",
        "outputId": "b0e22f70-20cb-43ab-c574-a38c989266df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.13 requires torch<2.2,>=1.10, but you have torch 2.2.0 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU torch transformers einops accelerate bitsandbytes streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNp-c04zt_rV"
      },
      "source": [
        "# Setting up the environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj2DPGQuAqGJ",
        "outputId": "ec3cb4d2-e20d-4e20-a57e-c27b696b2624"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".locks\n",
            "models--mlabonne--phixtral-2x2_8\n",
            "version.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "cache_directory = '/root/.cache/huggingface/hub'\n",
        "\n",
        "# Get the list of files and folders in the cache directory\n",
        "files_and_folders = os.listdir(cache_directory)\n",
        "\n",
        "# Print the list\n",
        "for item in files_and_folders:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81xMcNDcCXV2"
      },
      "source": [
        "# Setting up the streamlit app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL07ck6sCbYb"
      },
      "source": [
        "## Writing the app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OSRSWSE7E6Hq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6805e924-dbc7-4315-b2e0-fb588157b5fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Setup.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile Setup.py\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
        "from threading import Thread\n",
        "\n",
        "torch.set_default_device(\"cuda\")\n",
        "\n",
        "# Loading the tokenizer and model from Hugging Face's model hub.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"mlabonne/phixtral-2x2_8\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mlabonne/phixtral-2x2_8\",\n",
        "    torch_dtype=\"auto\",\n",
        "    load_in_8bit=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Defining a custom stopping criteria class for the model's text generation.\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        stop_ids = [50256, 50295]  # IDs of tokens where the generation should stop.\n",
        "        for stop_id in stop_ids:\n",
        "            if input_ids[0][-1] == stop_id:  # Checking if the last generated token is a stop token.\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "\n",
        "# Function to generate model predictions.\n",
        "def predict(message, history):\n",
        "    history_transformer_format = history + [[message, \"\"]]\n",
        "    stop = StopOnTokens()\n",
        "\n",
        "    # Formatting the input for the model.\n",
        "    system_prompt = \"<|im_start|>system\\nYou are Phixtral, a helpful AI assistant.<|im_end|>\"\n",
        "    messages = system_prompt + \"\".join([\"\".join([\"\\n<|im_start|>user\\n\" + item[0], \"<|im_end|>\\n<|im_start|>assistant\\n\" + item[1]]) for item in history_transformer_format])\n",
        "    input_ids = tokenizer([messages], return_tensors=\"pt\").to('cuda')\n",
        "    streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n",
        "    generate_kwargs = dict(\n",
        "        input_ids,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        top_k=50,\n",
        "        temperature=0.7,\n",
        "        num_beams=1,\n",
        "        stopping_criteria=StoppingCriteriaList([stop])\n",
        "    )\n",
        "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
        "    t.start()  # Starting the generation in a separate thread.\n",
        "    partial_message = \"\"\n",
        "    for new_token in streamer:\n",
        "        partial_message += new_token\n",
        "        if '<|im_end|>' in partial_message:  # Breaking the loop if the stop token is generated.\n",
        "            break\n",
        "        yield partial_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCIeoBMpCelZ",
        "outputId": "64876ce7-2957-4437-f57a-1c6bad4518a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile streamlit_app.py\n",
        "import os\n",
        "import streamlit as st\n",
        "from Setup import predict\n",
        "\n",
        "# App title\n",
        "st.set_page_config(page_title=\"Chatbot CLI\", page_icon=\"🤖\")\n",
        "\n",
        "st.title('🤖 Phixtral CLI')\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state['messages'] = []\n",
        "\n",
        "# Display chat messages\n",
        "for message in st.session_state.messages:\n",
        "    st.chat_message('human').write(message[0])\n",
        "    st.chat_message('ai').write(message[1])\n",
        "\n",
        "# User-provided prompt\n",
        "# Generate a new response if last message is not from assistant\n",
        "if prompt := st.chat_input(placeholder=\"Type a message...\"):\n",
        "    st.chat_message(\"human\").write(prompt)\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        message_placeholder = st.empty()\n",
        "        full_response = \"\"\n",
        "        # Simulate stream of response with milliseconds delay\n",
        "        for chunk in predict(prompt, st.session_state.messages):\n",
        "            full_response += chunk + \" \"\n",
        "            # Add a blinking cursor to simulate typing\n",
        "            message_placeholder.markdown(full_response + \"▌\")\n",
        "        message_placeholder.markdown(full_response)\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages[-1][1] = full_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb20OixNLYW0"
      },
      "source": [
        "## Running the app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fQh1M-v4LaXB"
      },
      "outputs": [],
      "source": [
        "!streamlit run streamlit_app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r--jQDi9LdsL",
        "outputId": "9b879ca9-bd7c-4f6d-a95f-b9de3b5f704c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.125.111.120\n",
            "Copy this IP into the webpage that opens below\n"
          ]
        }
      ],
      "source": [
        "!curl ipv4.icanhazip.com\n",
        "!echo \"Copy this IP into the webpage that opens below\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zApu16wlLgcb",
        "outputId": "685b7557-ae2c-48f4-9224-7fcd30221f2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.992s\n",
            "your url is: https://gentle-squids-shop.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPcAXfrHCZKt8+E12ih24X",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}